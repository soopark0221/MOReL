The Zen of Python, by Tim Peters

Beautiful is better than ugly.
Explicit is better than implicit.
Simple is better than complex.
Complex is better than complicated.
Flat is better than nested.
Sparse is better than dense.
Readability counts.
Special cases aren't special enough to break the rules.
Although practicality beats purity.
Errors should never pass silently.
Unless explicitly silenced.
In the face of ambiguity, refuse the temptation to guess.
There should be one-- and preferably only one --obvious way to do it.
Although that way may not be obvious at first unless you're Dutch.
Now is better than never.
Although never is often better than *right* now.
If the implementation is hard to explain, it's a bad idea.
If the implementation is easy to explain, it may be a good idea.
Namespaces are one honking great idea -- let's do more of those!
---------------- Beginning Dynamics Training ----------------
weight 1062922
 first_moment is -0.06792732328176498
second moment is 0.005031133070588112
new D is -0.0204208642244339
 first_moment is -0.07603269815444946
second moment is 0.006190374027937651
new D is -0.016210757195949554
 first_moment is -0.07880429923534393
second moment is 0.006540215108543634
new D is -0.008314810693264008
weight 1062922
D is [[ 0.00290572  0.00661112  0.00963017]
 [ 0.01013606  0.01869245  0.01426318]
 [-0.00064272  0.00146017  0.00298864]
 ...
 [-0.22430597 -0.37687355 -0.41142058]
 [-0.22297458 -0.39997897 -0.36551499]
 [-0.25497645 -0.45532942 -0.5789299 ]]
---------------- Ending Dynamics Training ----------------
---------------- Beginning Policy Training ----------------
---------------- Ending Policy Training ----------------
---------------- Successfully Completed Training ----------------
---------------- Beginning Policy Evaluation ----------------
Final total reward: [0, 0.0, 0.0, 0.0, 0, 20.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0, 0.0, 29.0, 13.0, 0, 0.0, 16.0, 0.0, 0, 0.0, 21.0, 0.0, 0, 21.0, 0.0, 0.0, 0, 14.0, 0.0, 0.0, 0, 0.0, 20.0, 0.0, 0, 0.0, 31.0, 29.0, 0, 23.0, 0.0, 27.0, 0, 19.0, 23.0, 15.0, 0, 0.0, 22.0, 0.0, 0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 19.0, 0, 21.0, 0.0, 26.0, 0, 0.0, 16.0, 0.0, 0, 17.0, 0.0, 20.0, 0, 13.0, 0.0, 0.0, 0, 0.0, 0.0, 0.0, 0, 32.0, 22.0, 19.0, 0, 0.0, 0.0, 0.0, 0, 19.0, 24.0, 0.0, 0, 21.0, 22.0, 18.0, 0, 0.0, 0.0, 0.0, 0, 25.0, 22.0, 18.0, 0, 21.0, 0.0, 0.0, 0, 15.0, 16.0, 0.0, 0, 0.0, 0.0, 25.0, 0, 0.0, 26.0, 21.0, 0, 0.0, 26.0, 0.0, 0, 15.0, 0.0, 0.0, 0, 0.0, 0.0, 25.0, 0, 21.0, 0.0, 0.0, 0, 16.0, 25.0, 0.0, 0, 0.0, 22.0, 20.0, 0, 23.0, 17.0, 0.0, 0, 0.0, 26.0, 12.0, 0, 0.0, 0.0, 0.0, 0, 25.0, 17.0, 0.0, 0, 0.0, 20.0, 0.0, 0, 0.0, 0.0, 0.0, 0, 0.0, 0.0, 17.0, 0, 0.0, 24.0, 20.0, 0, 0.0, 23.0, 0.0, 0, 0.0, 23.0, 12.0, 0, 0.0, 0.0, 22.0, 0, 0.0, 0.0, 28.0, 0, 0.0, 0.0, 0.0]
Final evaluation reward: 6.6
---------------- Ending Policy Evaluation ----------------
